{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 31: Variational Autoencoders II and GANs I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Build a Convolutional VAE for the EuroSAT dataset\n",
    "\n",
    "1. Demonstrate by plotting original and reconstructed images that your model is performing well. You are free to choose your model configuration and any hyperparameters\n",
    "2. Plot the reconstructed images from the mean of latent representations for 'industrial buildings'  and 'forest' classes\n",
    "3. Plot the transition from 'industrial buildings' to 'forest' by moving in the latent space from one class to another in 10 steps and generating the full images using the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper files\n",
    "from helpers.helper_utils import set_all_seeds\n",
    "from helpers.helper_data import get_dataloaders_eurosat\n",
    "from helpers.helper_models import Reshape, Trim\n",
    "from helpers.helper_train import train_cvae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Other settings\n",
    "DEVICE = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EuroSAT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloaders_eurosat(BATCH_SIZE, num_workers=8)\n",
    "\n",
    "# Verify dataset\n",
    "print('Train Dataset:')\n",
    "for images, labels in train_loader:\n",
    "    print(f'Image batch dimensions: {images.size()}')\n",
    "    print(f'Image label dimensions: {labels.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational Autoencoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer output size formula:\n",
    "$$o=\\lfloor\\frac{i+2p-k}{s}\\rfloor+1$$\n",
    "\n",
    "Transposed convolutional layer output size formula:\n",
    "$$o=s(i-1)+k-2p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 3 channels x 64x64 pixel images\n",
    "\n",
    "            # Output: 32 channels x 32x32 pixel images\n",
    "            nn.Conv2d(3, 32, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            # Output: 64 channels x 16x16 pixel images\n",
    "            nn.Conv2d(32, 64, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 128 channels x 8x8 pixel images\n",
    "            nn.Conv2d(64, 128, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 256 channels x 4x4 pixel images\n",
    "            nn.Conv2d(128, 256, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 512 channels x 2x2 pixel images\n",
    "            nn.Conv2d(256, 512, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 512x2x2 = 2048 features\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Fully connected layer: 2048 -> 512\n",
    "        self.z_mean = nn.Linear(2048, 512)\n",
    "\n",
    "        # Fully connected layer: 2048 -> 512\n",
    "        self.z_log_var = nn.Linear(2048, 512)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Fully connected layer: 512 -> 2048\n",
    "            nn.Linear(512, 2048),\n",
    "\n",
    "            # Reshape to 512x2x2 (how it was in encoder before flattening)\n",
    "            # See helper class above\n",
    "            Reshape(-1, 512, 2, 2),\n",
    "\n",
    "            # Output: 256 channels x 5x5 pixel images\n",
    "            nn.ConvTranspose2d(512, 256, stride=2, kernel_size=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 128 channels, 9x9 pixel images\n",
    "            nn.ConvTranspose2d(256, 128, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 64 channels, 17x17 pixel images\n",
    "            nn.ConvTranspose2d(128, 64, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 32 channels, 33x33 pixel images\n",
    "            nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "\n",
    "            # Output: 3 channels, 65x65 pixel images\n",
    "            nn.ConvTranspose2d(32, 3, stride=2, kernel_size=3, padding=1),\n",
    "\n",
    "            # Trim images from 65x65 to 64x64\n",
    "            # See helper class above\n",
    "            Trim(64),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encoding_fn(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        eps = torch.randn(z_mean.size(0), z_mean.size(1)).to(DEVICE)\n",
    "        z = z_mean + eps * torch.exp(z_log_var / 2.)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return encoded, z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE()\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = train_cvae(\n",
    "    model,\n",
    "    optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_loader=train_loader,\n",
    "    device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs6073-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "089acdbd81e861025207d6d06022757a580b22f8df3c6d0545687bfdaaf39b00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
