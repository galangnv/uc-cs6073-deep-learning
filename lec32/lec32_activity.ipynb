{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 32: GANs II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. The GAN model covered in the lecture used fully connected layers in discriminator and generator. In this exercise, use at least two convolutional layers in discriminator and two transposed convolutions in the generator. Train the model on MNIST data and tweak the hyperparameters as needed to do training effectively.\n",
    "2. Compare the results of your new GAN generator with the one in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper files\n",
    "from helpers.helper_utils import set_all_seeds\n",
    "from helpers.helper_data import get_dataloaders_mnist\n",
    "from helpers.helper_train import train_gan\n",
    "from helpers.helper_plots import plot_generated_images\n",
    "from helpers.helper_models import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "GENERATOR_LEARNING_RATE = 0.0002\n",
    "DISCRIMINATOR_LEARNING_RATE = 0.0002\n",
    "LATENT_DIM = 100\n",
    "\n",
    "# Other settings\n",
    "DEVICE = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 42\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "SAVED_DATA_PATH = '/Users/nxvgalang/Documents/main/college/cs6073/uc-cs6073-deep-learning/lec32/saved_data/'\n",
    "\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, uncomment this code\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# SAVED_DATA_PATH = 'drive/MyDrive/saved_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloaders_mnist(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_transforms=custom_transforms\n",
    ")\n",
    "\n",
    "# Verify dataset\n",
    "for images, labels in train_loader:\n",
    "    print(f'Image batch dimensions: {images.size()}')\n",
    "    print(f'Image label dimensions: {labels.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.title('Training Images')\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        torchvision.utils.make_grid(\n",
    "            images[:64],\n",
    "            padding=2,\n",
    "            normalize=True\n",
    "        ),\n",
    "        (1, 2, 0)\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model (from lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        image_height=28,\n",
    "        image_width=28,\n",
    "        color_channels=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.color_channels = color_channels\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(128, image_height * image_width * color_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(image_height * image_width * color_channels, 128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def generator_forward(self, z):\n",
    "        z = torch.flatten(z, start_dim=1)\n",
    "        img = self.generator(z)\n",
    "        img = img.view(\n",
    "            z.size(0),\n",
    "            self.color_channels,\n",
    "            self.image_height,\n",
    "            self.image_width\n",
    "        )\n",
    "        return img\n",
    "\n",
    "    def discriminator_forward(self, img):\n",
    "        logits = self.discriminator(img)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GAN()\n",
    "model1.to(DEVICE)\n",
    "\n",
    "generator_optimizer_1 = torch.optim.Adam(\n",
    "    model1.generator.parameters(),\n",
    "    betas=(0.5, 0.999),\n",
    "    lr=GENERATOR_LEARNING_RATE\n",
    ")\n",
    "\n",
    "discriminator_optimizer_1 = torch.optim.Adam(\n",
    "    model1.discriminator.parameters(),\n",
    "    betas=(0.5, 0.999),\n",
    "    lr=DISCRIMINATOR_LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict_1 = train_gan(\n",
    "    model1,\n",
    "    generator_optimizer_1,\n",
    "    discriminator_optimizer_1,\n",
    "    num_epochs=1,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    train_loader=train_loader,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), SAVED_DATA_PATH + 'model1.pt')\n",
    "\n",
    "# Can't save Tensor in JSON file, so convert to list before saving\n",
    "for i in range(len(log_dict_1['images_from_noise_per_epoch'])):\n",
    "    log_dict_1['images_from_noise_per_epoch'][i] = log_dict_1['images_from_noise_per_epoch'][i].tolist()\n",
    "\n",
    "with open(SAVED_DATA_PATH + 'logs1.json', 'w') as f:\n",
    "    json.dump(log_dict_1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DATA_PATH = '/Users/nxvgalang/Documents/main/college/cs6073/uc-cs6073-deep-learning/lec32/saved_data/'\n",
    "\n",
    "model1 = GAN()\n",
    "model1.load_state_dict(torch.load(SAVED_DATA_PATH + 'model1.pt', map_location=DEVICE))\n",
    "\n",
    "with open(SAVED_DATA_PATH + 'logs1.json', 'r') as f:\n",
    "    log_dict_1 = json.loads(f.read())\n",
    "\n",
    "# Convert image data back to Tensors\n",
    "for i in range(len(log_dict_1['images_from_noise_per_epoch'])):\n",
    "    log_dict_1['images_from_noise_per_epoch'][i] = torch.Tensor(log_dict_1['images_from_noise_per_epoch'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(log_dict=log_dict_1, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Deep Convolutional GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=100,\n",
    "        image_height=28,\n",
    "        image_width=28,\n",
    "        color_channels=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.color_channels = color_channels\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            # Input: 100 features\n",
    "\n",
    "            # Output: 784 features\n",
    "            nn.Linear(latent_dim, 784),\n",
    "            nn.BatchNorm1d(784),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # Output: 16 channels x 7x7 images\n",
    "            Reshape(-1, 16, 7, 7),\n",
    "\n",
    "            # Output: 32 channels x 14x14 images\n",
    "            nn.ConvTranspose2d(\n",
    "                16, 32,\n",
    "                kernel_size=5, stride=2, padding=2,\n",
    "                output_padding=1, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # Output: 1 channel x 28x28 images\n",
    "            nn.ConvTranspose2d(\n",
    "                32, 1,\n",
    "                kernel_size=5, stride=2, padding=2,\n",
    "                output_padding=1, bias=False\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # Input: 1 channel x 28x28 images\n",
    "\n",
    "            # Output: 32 channels x 14x14 images\n",
    "            nn.Conv2d(\n",
    "                1, 32,\n",
    "                kernel_size=5, stride=2, padding=2,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # Output: 16 channels x 7x7 images\n",
    "            nn.Conv2d(\n",
    "                32, 16,\n",
    "                kernel_size=5, stride=2, padding=2,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # Output: 784 features\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Output: 784 features\n",
    "            nn.Linear(784, 784),\n",
    "            nn.BatchNorm1d(784),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            # Output: 1 feature\n",
    "            nn.Linear(784, 1)\n",
    "        )\n",
    "    \n",
    "    def generator_forward(self, z):\n",
    "        z = torch.flatten(z, start_dim=1)\n",
    "        img = self.generator(z)\n",
    "        return img\n",
    "\n",
    "    def discriminator_forward(self, img):\n",
    "        logits = self.discriminator(img)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DCGAN()\n",
    "model2.to(DEVICE)\n",
    "\n",
    "generator_optimizer_2 = torch.optim.Adam(\n",
    "    model2.generator.parameters(),\n",
    "    betas=(0.5, 0.999),\n",
    "    lr=GENERATOR_LEARNING_RATE\n",
    ")\n",
    "\n",
    "discriminator_optimizer_2 = torch.optim.Adam(\n",
    "    model2.discriminator.parameters(),\n",
    "    betas=(0.5, 0.999),\n",
    "    lr=DISCRIMINATOR_LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict_2 = train_gan(\n",
    "    model2,\n",
    "    generator_optimizer_2,\n",
    "    discriminator_optimizer_2,\n",
    "    num_epochs=20,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    train_loader=train_loader,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), SAVED_DATA_PATH + 'model2.pt')\n",
    "\n",
    "# Can't save Tensor in JSON file, so convert to list before saving\n",
    "for i in range(len(log_dict_2['images_from_noise_per_epoch'])):\n",
    "    log_dict_2['images_from_noise_per_epoch'][i] = log_dict_2['images_from_noise_per_epoch'][i].tolist()\n",
    "\n",
    "with open(SAVED_DATA_PATH + 'logs2.json', 'w') as f:\n",
    "    json.dump(log_dict_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DATA_PATH = '/Users/nxvgalang/Documents/main/college/cs6073/uc-cs6073-deep-learning/lec32/saved_data/'\n",
    "\n",
    "model2 = DCGAN()\n",
    "model2.load_state_dict(torch.load(SAVED_DATA_PATH + 'model2.pt', map_location=DEVICE))\n",
    "\n",
    "with open(SAVED_DATA_PATH + 'logs2.json', 'r') as f:\n",
    "    log_dict_2 = json.loads(f.read())\n",
    "\n",
    "# Convert image data back to Tensors\n",
    "for i in range(len(log_dict_2['images_from_noise_per_epoch'])):\n",
    "    log_dict_2['images_from_noise_per_epoch'][i] = torch.Tensor(log_dict_2['images_from_noise_per_epoch'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(log_dict=log_dict_2, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs6073-dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "089acdbd81e861025207d6d06022757a580b22f8df3c6d0545687bfdaaf39b00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
